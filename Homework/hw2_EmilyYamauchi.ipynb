{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "round-subsection",
   "metadata": {},
   "source": [
    "## HW 2 \n",
    "Emily Yamauchi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-karaoke",
   "metadata": {},
   "source": [
    "Function $F$ is defined as :\n",
    "\n",
    "$$\n",
    "F(\\beta) = \\frac{1}{2n}\\sum_{i=1}^n(y_i-x_i^\\top \\beta)^2 + \\frac{\\lambda}{n}||\\beta||_2^2\n",
    "$$\n",
    "and the optimal solution as :\n",
    "$$\n",
    "F(\\beta^*) = \\min_{\\beta \\in \\mathbb{R}^d}F(\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-protection",
   "metadata": {},
   "source": [
    "1. Assume that $d = 1$ and $n = 1$. The sample size is then of size 1 and boils down to just $(x, y)$. The function $F$ writes simply as\n",
    "\n",
    "$$\n",
    "F(\\beta) = \\frac{1}{2}(y-x\\beta)^2 + \\lambda\\beta^2\n",
    "$$\n",
    "    Compute and write down the gradient $\\nabla{F}$ of $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-group",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F(\\beta) &= \\frac{1}{2}(y-x\\beta)^2 + \\lambda\\beta^2 \\\\\n",
    "\\frac{d}{d\\beta}F(\\beta) &= \\frac{1}{2}2(y-x\\beta)(-x)+2\\lambda\\beta \\\\\n",
    "&= -x(y-x\\beta) + 2\\lambda\\beta \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-deviation",
   "metadata": {},
   "source": [
    "2. Assume now $d > 1$ and $n > 1$. Using the previous result and the linearity of differentiation, compute and write down the gradient $\\nabla F(\\beta)$ of $F$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F(\\beta) &= \\frac{1}{2n}\\sum_{i=1}^n(y_i-x_i^\\top \\beta)^2 + \\frac{\\lambda}{n}||\\beta||_2^2 \\\\\n",
    "&= \\frac{1}{2n}(Y - X^\\top \\beta)^\\top (Y - X^\\top \\beta)+ \\frac{\\lambda}{n}\\beta^\\top \\beta \\\\\n",
    "&= \\frac{1}{2n}\\left(Y^\\top Y - Y^\\top X\\top \\beta - (X^\\top \\beta)^\\top Y + (X^\\top \\beta)^\\top X^\\top \\beta\\right) + \\frac{\\lambda}{n}\\beta^\\top I \\beta \\\\\n",
    "&= \\frac{1}{2n}(Y^\\top Y - Y^\\top X^\\top \\beta - \\beta^\\top XY + \\beta^\\top XX^\\top \\beta) + \\frac{\\lambda}{n}\\beta^\\top I \\beta \\\\\n",
    "\\frac{d}{d\\beta}F(\\beta)&=\\frac{1}{2n}[0-XY-XY+(XX^\\top + (XX^\\top)^\\top \\beta] + \\frac{\\lambda}{n}(I+I^\\top)\\beta \\\\\n",
    "&= \\frac{1}{2n}[-2XY+2(XX^\\top)\\beta] + \\frac{\\lambda}{n}2\\beta \\\\\n",
    "&= \\frac{-X(Y-X^\\top \\beta)}{n} + \\frac{2\\lambda\\beta}{n} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-baltimore",
   "metadata": {},
   "source": [
    "Consider the `Penguins` dataset, which you should load and divide into training and test sets using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "complex-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "heard-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "file = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv'\n",
    "penguins = pd.read_csv(file, sep=',', header=0)\n",
    "penguins = penguins.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "connected-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our X matrix with the predictors and y vector with the response\n",
    "\n",
    "X = penguins.drop('body_mass_g', axis=1)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = penguins['body_mass_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "logical-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the data into training and test sets. By default, 25% goes into the test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-fiber",
   "metadata": {},
   "source": [
    "Standardize the data. Note that you can convert a dataframe into an array by using `np.array()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "contained-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X_df):\n",
    "    \"\"\"\n",
    "    Apply standardization function to df type input X\n",
    "    Returns: standardized data in array type\n",
    "    \"\"\"\n",
    "\n",
    "    for col in X_df.columns:\n",
    "        X_df[col] = X_df[col] / np.std(X_df[col])\n",
    "    \n",
    "    return np.array(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "separated-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-latex",
   "metadata": {},
   "source": [
    "Write a function *computegrad* that computes and returns $\\nabla F(\\beta)$ for any $\\beta$. Avoid using `for` loops by vectorizing the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bright-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(beta, X, y, lamb):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the function given the beta, X and y vectors, and lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(X)\n",
    "    d1 = -X * (y + np.dot(X.T, beta)) / n\n",
    "    d2 = 2 * lamb/n * beta\n",
    "    \n",
    "    return d1 + d2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-passion",
   "metadata": {},
   "source": [
    "Write a function *graddescent* that implements the gradient descent algorithm described in Algorithm 1. The function *graddescent* calls the function *computegrad* as a subroutine. The function takes as input the initial point, the constant step-size value, and the maximum number of iterations. The stopping criterion is the maximum number of iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DATA558] *",
   "language": "python",
   "name": "conda-env-DATA558-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
