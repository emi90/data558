{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "round-subsection",
   "metadata": {},
   "source": [
    "## HW 2 \n",
    "Emily Yamauchi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-karaoke",
   "metadata": {},
   "source": [
    "Function $F$ is defined as :\n",
    "\n",
    "$$\n",
    "F(\\beta) = \\frac{1}{2n}\\sum_{i=1}^n(y_i-x_i^\\top \\beta)^2 + \\frac{\\lambda}{n}||\\beta||_2^2\n",
    "$$\n",
    "and the optimal solution as :\n",
    "$$\n",
    "F(\\beta^*) = \\min_{\\beta \\in \\mathbb{R}^d}F(\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-protection",
   "metadata": {},
   "source": [
    "1. Assume that $d = 1$ and $n = 1$. The sample size is then of size 1 and boils down to just $(x, y)$. The function $F$ writes simply as\n",
    "\n",
    "$$\n",
    "F(\\beta) = \\frac{1}{2}(y-x\\beta)^2 + \\lambda\\beta^2\n",
    "$$\n",
    "    Compute and write down the gradient $\\nabla{F}$ of $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-group",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F(\\beta) &= \\frac{1}{2}(y-x\\beta)^2 + \\lambda\\beta^2 \\\\\n",
    "\\frac{d}{d\\beta}F(\\beta) &= \\frac{1}{2}2(y-x\\beta)(-x)+2\\lambda\\beta \\\\\n",
    "&= -x(y-x\\beta) + 2\\lambda\\beta \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-deviation",
   "metadata": {},
   "source": [
    "2. Assume now $d > 1$ and $n > 1$. Using the previous result and the linearity of differentiation, compute and write down the gradient $\\nabla F(\\beta)$ of $F$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F(\\beta) &= \\frac{1}{2n}\\sum_{i=1}^n(y_i-x_i^\\top \\beta)^2 + \\frac{\\lambda}{n}||\\beta||_2^2 \\\\\n",
    "&= \\frac{1}{2n}(Y - X^\\top \\beta)^\\top (Y - X^\\top \\beta)+ \\frac{\\lambda}{n}\\beta^\\top \\beta \\\\\n",
    "&= \\frac{1}{2n}\\left(Y^\\top Y - Y^\\top X\\top \\beta - (X^\\top \\beta)^\\top Y + (X^\\top \\beta)^\\top X^\\top \\beta\\right) + \\frac{\\lambda}{n}\\beta^\\top I \\beta \\\\\n",
    "&= \\frac{1}{2n}(Y^\\top Y - Y^\\top X^\\top \\beta - \\beta^\\top XY + \\beta^\\top XX^\\top \\beta) + \\frac{\\lambda}{n}\\beta^\\top I \\beta \\\\\n",
    "\\frac{d}{d\\beta}F(\\beta)&=\\frac{1}{2n}[0-XY-XY+(XX^\\top + (XX^\\top)^\\top \\beta] + \\frac{\\lambda}{n}(I+I^\\top)\\beta \\\\\n",
    "&= \\frac{1}{2n}[-2XY+2(XX^\\top)\\beta] + \\frac{\\lambda}{n}2\\beta \\\\\n",
    "&= \\frac{-X(Y-X^\\top \\beta)}{n} + \\frac{2\\lambda\\beta}{n} \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-baltimore",
   "metadata": {},
   "source": [
    "Consider the `Penguins` dataset, which you should load and divide into training and test sets using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complex-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "heard-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "file = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv'\n",
    "penguins = pd.read_csv(file, sep=',', header=0)\n",
    "penguins = penguins.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "connected-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our X matrix with the predictors and y vector with the response\n",
    "\n",
    "X = penguins.drop('body_mass_g', axis=1)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = penguins['body_mass_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "logical-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the data into training and test sets. By default, 25% goes into the test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-fiber",
   "metadata": {},
   "source": [
    "Standardize the data. Note that you can convert a dataframe into an array by using `np.array()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "contained-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    \"\"\"\n",
    "    Apply standardization function to input data\n",
    "    Returns: standardized data in array type\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(data) == pd.DataFrame:\n",
    "        \n",
    "        for col in data.columns:\n",
    "            data[col] = data[col] / np.std(data[col])    \n",
    "        \n",
    "    elif type(data) == pd.Series:\n",
    "        \n",
    "        data = data/np.std(data)\n",
    "\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "separated-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eyamauchi/miniconda3/envs/DATA558/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#standardize(X_train)\n",
    "Xs = standardize(X_train)\n",
    "ys = standardize(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-latex",
   "metadata": {},
   "source": [
    "Write a function *computegrad* that computes and returns $\\nabla F(\\beta)$ for any $\\beta$. Avoid using `for` loops by vectorizing the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bright-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(beta, X, y, lamb):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the function given the beta, X and y vectors, and lambda\n",
    "    Returns: gradient of F\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(X)\n",
    "    d1 = -X * (y + np.dot(X.T, beta)) / n\n",
    "    d2 = 2 * lamb/n * beta\n",
    "    \n",
    "    return d1 + d2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-passion",
   "metadata": {},
   "source": [
    "Write a function *graddescent* that implements the gradient descent algorithm described in Algorithm 1. The function *graddescent* calls the function *computegrad* as a subroutine. The function takes as input the initial point, the constant step-size value, and the maximum number of iterations. The stopping criterion is the maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reported-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graddescent(eta, X, y, lamb, iternum):\n",
    "    \"\"\"\n",
    "    Implements the gradient calculations, iterates over specified number at given step size eta\n",
    "    Returns: List of betas\n",
    "    \"\"\"\n",
    "    \n",
    "    betas = [0.]\n",
    "    beta_i = 0.\n",
    "    for i in range(iternum):\n",
    "        grad = computegrad(beta_i, X, y, lamb)\n",
    "        beta_i = beta_i - eta * grad\n",
    "        betas.append(beta_i)\n",
    "    \n",
    "    return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-intelligence",
   "metadata": {},
   "source": [
    "Set the constant step-size $\\eta=0.5$ and the maximum number of iterations to 1000. Run *graddescent* on the training set of the `Penguins` dataset for $\\lambda=-5.00$. Plot the curve of the objective value $F(\\beta_t)$ versus the iteration counter $t$. Again, avoid using for loops when computing the objective values. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "infinite-queen",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (249,8) (8,249) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-17800e7923c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5.00\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraddescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miternum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-628796508087>\u001b[0m in \u001b[0;36mgraddescent\u001b[0;34m(eta, X, y, lamb, iternum)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbeta_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miternum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputegrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mbeta_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_i\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbetas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-22fa7db0d0e2>\u001b[0m in \u001b[0;36mcomputegrad\u001b[0;34m(beta, X, y, lamb)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (249,8) (8,249) "
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "iternum = 1000\n",
    "lamb = -5.00\n",
    "\n",
    "betas = graddescent(eta, Xs, ys, lamb, iternum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DATA558] *",
   "language": "python",
   "name": "conda-env-DATA558-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
