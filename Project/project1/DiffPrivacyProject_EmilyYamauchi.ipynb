{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thorough-indonesia",
   "metadata": {},
   "source": [
    "# <center>Data Science Project Part 1:  Differential Privacy</center>\n",
    "<center>DATA 558, Spring 2021</center>\n",
    "<center>TAs: Alec Greaves-Tunnell and Ronak Mehta</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-tracy",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-brass",
   "metadata": {},
   "source": [
    "#### Name:  Emily Yamauchi\n",
    "#### Partner:  Samuel Perebikovsky\n",
    "#### Summary of findings:\n",
    "\n",
    "    A: Preprocessing: Should exclude testing set from `database`\n",
    "                      Should binary variables get standardized?\n",
    "                      Should X_test be excluded from standardization scale?\n",
    "\n",
    "    B: Clearly there should be difference between the three methods, as Mech1 only passes the variables, whereas Mech2 and Mech3 adds noise.\n",
    "    Posterior calculation: numerator should be P(label|flipper)*P(flipper)\n",
    "    Conditional probability setup: See below.\n",
    "    \n",
    "    C: The three methods differ when the above two steps are implemented. Mech2 is approx. 5% vs 12% prior probability, whereas Mech1 and Mech3 are 0% (no privacy maintained?)\n",
    "    When conditional probability setup change is implemented, prior is 87% (i.e. P(no flipper)), posterior is 100%, 97%, 100% respectively.\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reserved-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-malaysia",
   "metadata": {},
   "source": [
    "## Dataset: Animals with Attributes v2\n",
    "\n",
    "The data comes from Animals with Attributes v2 dataset, which contains images of 50 different types of animals, with each animal labeled with various attributes (whether it flies, whether it has a tail, etc.). In this example, we will not use the images, and treat the classes themselves as datapoints. That is, we will have $n = 50$ data points $(x_1, y_1), ..., (x_n, y_n)$, where $x_i \\in \\{0, 1\\}^d$ is a binary vector of attributes, and $y_i \\in \\{0, 1\\}$ is a binary label indicating whether the animal is an `ocean` animal or not. There were originally 85 attributes, but we have subsetted them to $d = 5$ features, namely:\n",
    "\n",
    "- `horns` - whether the animal has horns.\n",
    "- `tree` - whether the animal lives in a tree.\n",
    "- `bulbous` - whether the animal is stocky.\n",
    "- `fierce` - whether the animal is fierce.\n",
    "- `arctic` - whether the animal lives in the arctic.\n",
    "\n",
    "Additionally, we have one protected attribute, `flippers`, indicating whether the animal has flippers. This attribute is known for 49 of the animals, but is unknown for a held out animal, the `buffalo`. We will inspect in this notebook whether we can uncover the `buffalo`'s protected attribute using a machine learning model trained on the 49 points. If the model is privacy preserving, we should not be able to do this any better than if we did not have the model in hand. If not, then we will be significantly more confident by virtue of having access to the (outputs of the) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "japanese-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"privacy_data.pkl\", \"rb\"))\n",
    "\n",
    "X = data['X'].to_numpy() \n",
    "y = data['y'].to_numpy() # ocean\n",
    "attr = data['z'].to_numpy()   # The attributes of the database entries. \n",
    "x = data['animal'].to_numpy() # The targeted individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ignored-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data['X'].reset_index().sort_values(by = 1).rename(columns = {1:'animal'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-chance",
   "metadata": {},
   "source": [
    "The analysis you are expected to critique begins below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-surgery",
   "metadata": {},
   "source": [
    "## <center>Differentially Private Logistic Regression for AwA2</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-abuse",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Standard preprocessing techniques are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legendary-massage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (39, 5)\n",
      "y train shape: (39,)\n",
      "X test shape: (10, 5)\n",
      "y test shape: (10,)\n",
      "attr train shape:  (39,)\n",
      "attr test shape:  (10,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "X_train, X_test, y_train, y_test, attr_train, attr_test = train_test_split(X, y, attr, test_size=0.2)\n",
    "scaler = StandardScaler().fit(X)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"X train shape:\", X_train.shape)\n",
    "print(\"y train shape:\", y_train.shape)\n",
    "print(\"X test shape:\", X_test.shape)\n",
    "print(\"y test shape:\", y_test.shape)\n",
    "print(\"attr train shape: \", attr_train.shape)\n",
    "print(\"attr test shape: \", attr_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-board",
   "metadata": {},
   "source": [
    "##### Comment 1:  \n",
    "- `StandardScaler().fit()` should be called on `X_train` to avoid scaling on entire dataset (i.e. peeking at the test set).\n",
    "  However-- Lab4 also called the `StandardScaler().fit()` to the entire `X` set, so not confident on this critique.\n",
    "- Do binary inputs need to be scaled?\n",
    "- `z` also needs to be split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "corporate-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-terminal",
   "metadata": {},
   "source": [
    "## The Privacy Mechanism\n",
    "\n",
    "Below, we will implement the mechanism that returns responses from the machine learning model given queries from the data analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "biblical-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class Mechanism:\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self, database, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def respond(query):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "matched-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-privacy",
   "metadata": {},
   "source": [
    "First, we cover the machine learning model. We will use unregularied logistic regression to map the feature vector $x \\in \\{0, 1\\}^5$ to its label $y \\in \\{0, 1\\}$. The `query` from the data analyst can come in four forms.\n",
    "- `all` - indicating that the mechanism should return responses (predicted labels) for all 49 training points in the database.\n",
    "- `flippers` - indicating that the mechanism should return responses (predicted labels) for all training points in the database that have flippers.\n",
    "- `no_flippers` - indicating that the mechanism should return responses (predicted labels) for all training points in the database that do not have flippers.\n",
    "- `x` - a single feature vector to be predicted by the model, passed as a `numpy.ndarray`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-spirituality",
   "metadata": {},
   "source": [
    "We implement three mechanisms, each of which will produce the response in different ways while preserving privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "limiting-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mechanism1(Mechanism):\n",
    "    \n",
    "    def __init__(self, database, attr):\n",
    "        self.X, self.y = database\n",
    "        self.model = LogisticRegression().fit(self.X, self.y)\n",
    "        self.attr = attr\n",
    "        self.name = \"Mechanism 1\"\n",
    "        \n",
    "    def respond(self, query):\n",
    "        if isinstance(query, np.ndarray):\n",
    "            return self.model.predict(query.reshape(1, -1))[0]\n",
    "        elif query == \"all\":\n",
    "            return self.model.predict(self.X)\n",
    "        elif query == \"flippers\":\n",
    "            return self.model.predict(self.X[self.attr == 1])\n",
    "        elif query == \"no_flippers\":\n",
    "            return self.model.predict(self.X[self.attr == 0])\n",
    "        else:\n",
    "            raise ValueError(\"'query' must be 'all', 'flippers', 'no_flippers', or a numpy.ndarry object.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-hollow",
   "metadata": {},
   "source": [
    "##### Comment 2:  \n",
    "`Mechanism1` has no privacy protection- passes each variable through the class with no noise added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "flush-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mechanism2(Mechanism):\n",
    "    \n",
    "    def __init__(self, database, attr, prob=0.9):\n",
    "        self.X, self.y = database\n",
    "        self.model = LogisticRegression().fit(self.X, self.y)\n",
    "        self.attr = attr\n",
    "        self.name = \"Mechanism 2\"\n",
    "        self.prob = prob\n",
    "        \n",
    "    def respond(self, query):\n",
    "        if isinstance(query, np.ndarray):\n",
    "            coin = np.random.binomial(1, self.prob)\n",
    "            if coin == 1:\n",
    "                return self.model.predict(query.reshape(1, -1))[0]\n",
    "            else:\n",
    "                return np.random.binomial(1, 0.5)\n",
    "        elif query == \"all\":\n",
    "            X = self.X\n",
    "        elif query == \"flippers\":\n",
    "            X = self.X[self.attr == 1]\n",
    "        elif query == \"no_flippers\":\n",
    "            X = self.X[self.attr == 0]\n",
    "        else:\n",
    "            raise ValueError(\"'query' must be 'all', 'flippers', 'no_flippers', or a numpy.ndarry object.\")\n",
    "            \n",
    "        y = self.model.predict(X)\n",
    "        coins = np.random.binomial(1, self.prob, size=y.shape)\n",
    "        random_response = np.random.binomial(1, 0.5, size=y.shape)\n",
    "        \n",
    "        return y * coins + random_response * (1 - coins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-wrestling",
   "metadata": {},
   "source": [
    "##### Comment 3:   \n",
    "`Mechanism2` adds noise to the output directly. Adds a weighted coin-toss at default 90% to the output. Possible critique-- is this enough nosie? TBD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hollow-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "class Mechanism3(Mechanism):\n",
    "    \n",
    "    def __init__(self, database, attr, scale=1):\n",
    "        self.X, self.y = database\n",
    "        self.model = LogisticRegression().fit(self.X, self.y)\n",
    "        self.attr = attr\n",
    "        self.name = \"Mechanism 3\"\n",
    "        self.scale = scale\n",
    "        \n",
    "    def respond(self, query):\n",
    "        if isinstance(query, np.ndarray):\n",
    "            noise = np.random.uniform(-0.1, 0.1, size=query.shape)\n",
    "            return  self.model.predict((query + noise).reshape(1, -1))[0]\n",
    "        elif query == \"all\":\n",
    "            X = self.X\n",
    "        elif query == \"flippers\":\n",
    "            X = self.X[self.attr == 1]\n",
    "        elif query == \"no_flippers\":\n",
    "            X = self.X[self.attr == 0]\n",
    "        else:\n",
    "            raise ValueError(\"'query' must be 'all', 'flippers', 'no_flippers', or a numpy.ndarry object.\")\n",
    "            \n",
    "        X = X + np.random.uniform(-0.1, 0.1, size=X.shape)\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-petite",
   "metadata": {},
   "source": [
    "##### Comment 4:  \n",
    "`Mechanism3` adds noise to the input variable as uniform distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mexican-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = (X, y)\n",
    "\n",
    "m1 = Mechanism1(database, attr)\n",
    "m2 = Mechanism2(database, attr)\n",
    "m3 = Mechanism3(database, attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spiritual-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fix = (X_train, y_train)\n",
    "m1_fix = Mechanism1(data_fix, attr_train)\n",
    "m2_fix = Mechanism2(data_fix, attr_train)\n",
    "m3_fix = Mechanism3(data_fix, attr_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-nutrition",
   "metadata": {},
   "source": [
    "##### Comment 5:  \n",
    "`Database` should consist of just `X_train`, `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "operational-ivory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1.respond(x):  0\n",
      "m1_fix.respond(x):  0\n"
     ]
    }
   ],
   "source": [
    "print('m1.respond(x): ', m1.respond(x))\n",
    "print('m1_fix.respond(x): ', m1_fix.respond(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "steady-small",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m2.respond(x):  0\n",
      "m2_fix.respond(x):  0\n"
     ]
    }
   ],
   "source": [
    "print('m2.respond(x): ', m2.respond(x))\n",
    "print('m2_fix.respond(x): ', m2_fix.respond(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "artistic-mentor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m3.respond(x):  0\n",
      "m3_fix.respond(x):  0\n"
     ]
    }
   ],
   "source": [
    "print('m3.respond(x): ', m3.respond(x))\n",
    "print('m3_fix.respond(x): ', m3_fix.respond(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-michigan",
   "metadata": {},
   "source": [
    "## The Advarsarial Attack\n",
    "\n",
    "We justify our claim of privacy by showing that an attack fails. In other words, we want the probabilities surrounding whether the targetted animal (`buffalo`) having the attribute `flipper` to not change very much given the response $\\hat{y}$ from the mechanism. The prior probability (that is, prior to calling the mechanism) of having `flippers` is estimated using the number of animals in the training set that have flippers, which we assume is known by the attacker. We want:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left(\\text{flippers}\\mid \\hat{y}(\\text{buffalo}) =\\text{ocean}\\right) \\approx \\mathbb{P}\\left(\\text{flippers}\\right)\n",
    "$$\n",
    "\n",
    "If we are much more confident about the value of this protected attribute (i.e. the probabilities are significantly higher or lower) after using the prediction for the `ocean` attribute of `buffalo` from the model, then the mechanism has failed to protect privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "announced-relations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: 0.143\n",
      "Mechanism 1 Posterior: 0.167\n",
      "Mechanism 2 Posterior: 0.167\n",
      "Mechanism 3 Posterior: 0.166\n"
     ]
    }
   ],
   "source": [
    "# Use Bayes rule to get a value for the probability of `x` having attr == flippers.\n",
    "np.random.seed(123)\n",
    "\n",
    "prior = attr.mean()\n",
    "print(\"Prior: %0.3f\" % prior)\n",
    "\n",
    "for mech in [m1, m2, m3]:\n",
    "    label = mech.respond(x)\n",
    "    # print(label)\n",
    "\n",
    "    prob_1_given_attr_1 = mech.respond(\"flippers\").mean()\n",
    "    # print(prob_1_given_attr_1)\n",
    "    prob_1_given_attr_0 = mech.respond(\"no_flippers\").mean()\n",
    "    # print(prob_1_given_attr_0)\n",
    "\n",
    "    if label == 1:\n",
    "        prob_label_given_attr_1 = prob_1_given_attr_1\n",
    "        prob_label_given_attr_0 = prob_1_given_attr_0\n",
    "    else:\n",
    "        prob_label_given_attr_1 = prob_1_given_attr_0\n",
    "        prob_label_given_attr_0 = prob_1_given_attr_1\n",
    "\n",
    "    posterior = prior * prob_label_given_attr_0 / (prior * prob_label_given_attr_1 + (1 - prior) *  prob_label_given_attr_0)\n",
    "\n",
    "    print(\"%s Posterior: %0.3f\" % (mech.name, posterior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "magnetic-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: 0.128\n",
      "Mechanism 1 Posterior: 0.004\n",
      "Mechanism 2 Posterior: 0.047\n",
      "Mechanism 3 Posterior: 0.005\n"
     ]
    }
   ],
   "source": [
    "# Use Bayes rule to get a value for the probability of `x` having attr == flippers.\n",
    "#np.random.seed(123)\n",
    "# Use simulation instead. Fix posterior calc only. Use trainset\n",
    "\n",
    "prior = attr_train.mean()\n",
    "print(\"Prior: %0.3f\" % prior)\n",
    "\n",
    "for mech in [m1_fix, m2_fix, m3_fix]:\n",
    "    post = []\n",
    "    \n",
    "    for i in range(1000):\n",
    "        label = mech.respond(x)\n",
    "        # print(label)\n",
    "\n",
    "        prob_1_given_attr_1 = mech.respond(\"flippers\").mean()\n",
    "        # print(prob_1_given_attr_1)\n",
    "        prob_1_given_attr_0 = mech.respond(\"no_flippers\").mean()\n",
    "        # print(prob_1_given_attr_0)\n",
    "\n",
    "        if label == 1:\n",
    "            prob_label_given_attr_1 = prob_1_given_attr_1\n",
    "            prob_label_given_attr_0 = prob_1_given_attr_0\n",
    "        else:\n",
    "            prob_label_given_attr_1 = prob_1_given_attr_0\n",
    "            prob_label_given_attr_0 = prob_1_given_attr_1\n",
    "\n",
    "        post_calc = prior * prob_label_given_attr_1 / (prior * prob_label_given_attr_1 + (1 - prior) *  prob_label_given_attr_0)\n",
    "        post.append(post_calc)\n",
    "    \n",
    "    posterior = np.mean(np.array(post))\n",
    "    print(\"%s Posterior: %0.3f\" % (mech.name, posterior))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-projector",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P(\\text{flippers}|\\text{label})&=\\frac{P(\\text{label}|\\text{flippers})\\times P(\\text{flippers})}{P(\\text{label}|\\text{flippers})\\times P(\\text{flippers})+P(\\text{label}|\\text{no flippers})\\times P(\\text{no flippers})}\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\\\ \\\\ \\\\ \n",
    "\\small\\text{Case label == 1:}\\\\ \n",
    "P(\\text{flippers}|\\text{ocean})&=\\frac{P(\\text{ocean}|\\text{flippers})\\times P(\\text{flippers})}{P(\\text{ocean}|\\text{flippers})\\times P(\\text{flippers})+P(\\text{ocean}|\\text{no flippers})\\times P(\\text{no flippers})}\n",
    "\\\\ \\\\ \\\\ \n",
    "\\small\\text{Case label == 0:}\\\\ \n",
    "P(\\text{no flippers}|\\text{no ocean})&=\\frac{P(\\text{no ocean}|\\text{no flippers})\\times P(\\text{no flippers})}{P(\\text{no ocean}|\\text{no flippers})\\times P(\\text{no flippers})+P(\\text{no ocean}|\\text{flippers})\\times P(\\text{flippers})}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "smart-skirt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: 0.872\n",
      "Mechanism 1 Posterior: 1.000\n",
      "\n",
      "\n",
      "Prior: 0.872\n",
      "Mechanism 2 Posterior: 0.977\n",
      "\n",
      "\n",
      "Prior: 0.872\n",
      "Mechanism 3 Posterior: 1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix calc?\n",
    "#np.random.seed(123)\n",
    "\n",
    "flippers = attr_train.mean() #P(flippers)\n",
    "\n",
    "\n",
    "\n",
    "for mech in [m1_fix, m2_fix, m3_fix]:\n",
    "    post = []\n",
    "    \n",
    "    for i in range(1000):\n",
    "        label = mech.respond(x) #predict if buffalo is ocean based on mechanism 1-3\n",
    "        #print(\"label (buffalo = ocean): \", label)\n",
    "\n",
    "        given_flippers = mech.respond(\"flippers\").mean() #P(ocean|flippers)\n",
    "        given_not_flippers = mech.respond(\"no_flippers\").mean() #P(ocean|no flippers)\n",
    "\n",
    "        if label == 1: # label = ocean (case 1)\n",
    "            label_given_f = given_flippers\n",
    "            not_label_given_f = given_not_flippers\n",
    "            prior = flippers\n",
    "\n",
    "        else: # label = not ocean (case 0)\n",
    "            label_given_f = 1- given_not_flippers\n",
    "            not_label_given_f = 1- given_flippers\n",
    "            prior = 1 - flippers\n",
    "\n",
    "        post_calc = prior*label_given_f / (prior*label_given_f + (1-prior)*not_label_given_f)\n",
    "        post.append(post_calc)\n",
    "        #posterior = post_calc\n",
    "    \n",
    "    posterior = np.array(post).mean()\n",
    "    print(\"Prior: %0.3f\" % prior)\n",
    "    print(\"%s Posterior: %0.3f\" % (mech.name, posterior))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-bidder",
   "metadata": {},
   "source": [
    "Clearly, these probabilities have not changed very much from the prior. Thus, we can be assured that each mechanism preserves privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-accommodation",
   "metadata": {},
   "source": [
    "The three mechanisms should have different results, or at least, Mech1 has no privacy masking mechanism. As shown above, Mech2 has some privacy masking mechanism, while Mech1 and Mech3 do not. Therefore, masking the input only is not sufficient to maintain privacy, but the output variable themselves must have noise injected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-elephant",
   "metadata": {},
   "source": [
    "## Maintaining Accuracy\n",
    "\n",
    "Trivially, we can always preserve privacy by injecting a sufficiently large amount of noise. While this may be good by one metric, we might completely destroy the predictive performance of the model! It is important to maintain a balance such that we still perform well on a test set, which we inspect below. Note that this step would normally be done on a validation set, and final performance would be computed on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "loaded-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "english-words",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mechanism 1 Test Accuracy: 0.90\n",
      "Mechanism 2 Test Accuracy: 0.80\n",
      "Mechanism 3 Test Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "for mech in [m1, m2, m3]:\n",
    "    y_pred = np.array([mech.respond(x) for x in X_test])\n",
    "    print(\"%s Test Accuracy: %0.2f\" % (mech.name, accuracy_score(y_pred, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
